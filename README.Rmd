---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

<img src="man/figures/logo.png" align="right" height="130" alt="" />

# AnimalTrackR

AnimalTrackR provides a simple interface to YOLOv8 to allow users to train animal detection models to aid large scale behavioural analyses.

*The package is still in development and most features are not yet available.*

## Installation

You can install the development version of AnimalTrackR like so (you will need the devtools package installed):

``` {r install-package, eval=FALSE, echo = TRUE}
devtools::install_github("mariolambrette/AnimalTrackR")

library(AnimalTrackR)
```

FFmpeg is a critical dependency of `AnialTrackR`. Please ensure that you have FFmpeg installed on your machine before attempting to use `AnimalTrackR` for image analysis. To install go to this link: https://ffmpeg.org/download.html and follow the instructions to install the correct version of FFmpeg for your operating system.

## Intructions

### Overview

`AnimalTrackR` provides a simple end-to-end pipeline for using [YOLOv8 detection models](https://docs.ultralytics.com/tasks/detect/) to track an individual animal in captivity for behavioural analyses. Broadly, it will allow you to sample frames from the total of your experiment's footage which you can then annotate and use to train a YOLOv8 detection model. You can then run this model on short example videos for display and validation or in batch on large quantities of footage to allow subsequent analyses. 

#### File Structure

`AnimalTrackR` uses a specific file structure to support working on multiple projects and allow seamless integration with standard YOLOv8 development pipelines. The structure is as follows:

```
.
└── AnimalTrackR-projects
    ├── Project1
    │   ├── ToAnnotate
    │   └── YOLO
    │       ├── Train
    │       │   ├── labels
    │       │   │   └── .txt annotation files corresponding to the images in the images/ directory
    │       │   └── images
    │       │       └── .jpg images ready for model training
    │       ├── Test
    │       │   ├── labels
    │       │   └── images
    │       ├── Val
    │       │   ├── labels
    │       │   └── images
    │       ├── configs
    │       │   ├── model1.yaml 
    │       │   │   └── model training configuration files, there will be one for each model in the models/ directory
    │       │   └── model2.yaml
    │       └── models
    │           ├── model1/
    │           │   └── Files containing model training info and model weights. 
    │           └── model2/
    └── Project2
        └── ...
```
        
Most users will never need to interact directly with the vast majority of this file structure, it is created and managed by helper functions provided in `AnimalTrackR`


### First Usage:

AnimalTrackR relies heavily on python to execute image processing tasks. Specifically, it uses a python 3.9 virtual environment. This means that when you first install the package you will need to run the following command to create this environment:

```{r install-TrackR, eval=FALSE, echo = TRUE}

library(AnimalTrackR)

install_TrackR()

```

This relies on you having python 3.9 installed on your machine. The above function will give an error if you don't. You can install python 3.9 using any of the common installation techniques, though the simplest method for this application is to run the following code in your R session to install the latest release of python 3.9 into your root directory:

```{r install-python, eval=FALSE, echo=TRUE}

library(reticulate)

install_python(version = "3.9:latest")

```

These steps are only required the first time you use AnimalTrackR. The correct environment is now set up and will be made available to your R session with any subsequent usage of ```library(AnimalTrackR)```.

### Starting a project

Now that you have attached `AnimalTrackR` to your R session and created a suitable python environment to handle image processing tasks you will need to start a project. An `AnimalTrackR` project contained within a directory that has a specific structure to support the image annotation process and allow for seamless integration with other YOLO development pipelines. See the FILE STRUCTURE DOCUMENTATION FOR MORE INFORMATION. use the `init_Project()` function to initiate a new project by providing the path to the project root directory. Providing just a project name (as in the example below) will create a project folder with that name in the current working directory.

```{r init-project, eval=FALSE, echo=TRUE}

init_Project(path = 'MyProject')

```


Now that your project is set up you are ready to start developing a detection model.

### Extracting training images

The first step is to extract still images from your experimental footage. This step will require careful consideration of your experimental design and the footage you have. To have the best chance of developing accurate detection models the training images used should accurately reflect variability in your experimental data. For some experiments with very little variability between video clips this may be very straightforward but for others where there may be many different tank/cage set ups and variable backgrounds this may be more complex.

The basic premise for this step is to create a stratified sample with the different experimental groups. Due to the potential for massive variation in user's file structures and experimental designs the process of grouping video files sensibly is left to users. The image extraction function `extract_Images()`(described in detail further down) takes a named list as in input. This list's structure is as follows:

```{r vidfile-list, eval=FALSE, echo=TRUE}

video_files <- list(
  Group1 = list(
    "video1.mp4",
    "video2.mp4",
    "video3.mp4",
    ...
  ),
  Group2 = list(
    "video4.mp4",
    "video5.mp4",
    "video6.mp4",
    ...
  ),
  ...
)


```

The `extract_Images()` also takes another list as an argument which defines the weights to apply to each group when performing a stratified sample. This argument is optional, by default the function will use the relative numbers of videos in each group to inform the weights to apply, however there may be instances where users want to change this. This argument to do this should be of the same structure as the `video_files` list:

```{r group-weights, eval=FALSE, echo=TRUE}

group_weights <- list(
  Group1 = 0.4,
  Group2 = 0.6
)

```

The sum of the weights supplied must be equal to 1 and the names of the groups must be the same in these two lists or `extract_Images()` will throw an error.

Run `extract_Images()` as follows:

```{r extractImages, eval=FALSE, echo=TRUE}

extract_Images(
  videos = video_files,
  group_weights = group_weights,
  nimgs = 1600
)

```

The `nimgs` argument defines the total number of images that will be exported from the specified video. The default number (1600) will provide around 1000 training images and 300 images each for testing and validation. In many cases this will be sufficient to train highly effective models, but more complex experimental scenes may require a larger number of training images, particularly if the backgrounds employed are highly variable. The recommendation is to start with the default number and start training detection models, then add further images if model performance is poor.

`extract_Images()` will populate the 'ToAnnotate' folder within the project directory. Once this has happened you are ready to start annotating images for training and testing detection models.

### Image annotation














